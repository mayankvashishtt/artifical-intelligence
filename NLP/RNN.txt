Word2Vec DOES embeddings
It learns vectors so similar words are close (king‚Äìqueen vibes).
That‚Äôs why it‚Äôs better than BOW / TF-IDF.

RNN does NOT do word embeddings
This is key. RNN doesn‚Äôt learn word meaning directly.
What RNN actually does:
You feed embeddings INTO RNN (from Word2Vec/GloVe/etc).
RNN then learns context + order over time.

Classic NLP pipeline was:
Text ‚Üí Word Embeddings ‚Üí RNN ‚Üí Output

ann network?


Introduction to Recurrent Neural Networks (RNNs)
Problems with traditional neural networks for sequential data
RNN architecture and how it addresses temporal dependencies
Types of RNN architectures (one-to-one, one-to-many, many-to-one, many-to-many)
Word embeddings in RNNs
Mathematical formulation of RNNs (forward pass)
Backpropagation through time (BPTT) for RNNs
Vanishing and exploding gradient problems in RNNs
Basic implementation of RNN using TensorFlow/Keras
Introduction to Long Short-Term Memory (LSTM) networks (brief mention)
The session focused heavily on the mathematical foundations of RNNs, including the forward pass and backpropagation through time. The instructor also emphasized the importance of understanding these concepts deeply and encouraged students to practice the mathematics by hand.



# Recurrent Neural Networks (RNN) & LSTM ‚Äî Complete Notes

These notes summarize **everything we discussed step‚Äëby‚Äëstep**, from intuition to math to code, in a clean, exam‚Äëready way.

---

## 1. Why RNNs Exist

### Problem with Traditional Neural Networks (ANN)

* ANN processes inputs **independently**.
* It has **no memory** of previous inputs.
* Word order is ignored.

Example:

* "dog bites man" = "man bites dog" for ANN ‚ùå

ANN fails for:

* Text
* Speech
* Time series

---

## 2. What is an RNN?

**RNN = Recurrent Neural Network**

Key idea:

> RNN processes data **step by step** and carries information forward using memory.

* Designed for **sequential data**
* Uses the same weights at every time step
* Maintains a **hidden state** (memory)

---

## 3. RNN Architecture (Core Idea)

At each time step `t`:

* Input: `x‚Çú`
* Previous memory: `h‚Çú‚Çã‚ÇÅ`
* Output memory: `h‚Çú`

Forward logic:

```
h‚Çú = tanh(W‚Çì x‚Çú + W‚Çï h‚Çú‚Çã‚ÇÅ + b)
```

* `h‚Çú` = hidden state (short‚Äëterm memory)
* Memory flows **forward in time**

---

## 4. Types of RNN Architectures

| Type        | Example                      |
| ----------- | ---------------------------- |
| One ‚Üí One   | Image ‚Üí label (ANN, not RNN) |
| Many ‚Üí One  | Sentence ‚Üí sentiment         |
| One ‚Üí Many  | Image ‚Üí caption              |
| Many ‚Üí Many | Translation, text generation |

**Rule:**

> Use RNN if **input or output is a sequence**.

---

## 5. Word Embeddings in RNN

Neural networks **cannot read text**.

Pipeline:

```
Text ‚Üí Tokenizer ‚Üí Numbers ‚Üí Embedding ‚Üí RNN ‚Üí Output
```

### Embedding Layer

* Converts word indices into dense vectors
* Learns semantic meaning automatically

Example:

```
"good" ‚Üí [0.12, -0.44, 0.89, ...]
```

RNN **does not care** how embeddings are created.
It only processes vectors.

---

## 6. Forward Propagation (RNN)

Forward pass = **prediction only** (no learning).

Steps:

1. Start with `h‚ÇÄ = 0`
2. For each word:

```
h‚Çú = tanh(W‚Çï h‚Çú‚Çã‚ÇÅ + W‚Çì x‚Çú + b)
```

3. Final output:

```
y = W·µß h_T
```

---

## 7. Backward Propagation Through Time (BPTT)

Backward pass = **learning phase**.

* Error is calculated at final output
* Error flows **backward through all time steps**

Main BPTT equation:

```
‚àÇL / ‚àÇW‚Çï = Œ£ (t = 1 to T) ‚àÇL/‚àÇh‚Çú ¬∑ ‚àÇh‚Çú/‚àÇW‚Çï
```

Meaning:

> The recurrent weight is blamed using contributions from **all time steps**.

---

## 8. Jacobians in RNN

Jacobian:

```
J‚Çú = ‚àÇh‚Çú / ‚àÇh‚Çú‚Çã‚ÇÅ
```

* Measures how current memory depends on previous memory
* During BPTT, Jacobians are **multiplied repeatedly**

Backward gradient flow:

```
‚àÇL/‚àÇh‚ÇÅ = ‚àÇL/‚àÇh_T √ó J_T √ó J_{T-1} √ó ... √ó J_2
```

---

## 9. Vanishing & Exploding Gradient Problem

### Vanishing Gradient ‚ùÑÔ∏è

* Jacobian values < 1
* Repeated multiplication ‚Üí gradients ‚Üí 0
* Early time steps get **no learning**

### Exploding Gradient üí•

* Jacobian values > 1
* Gradients grow uncontrollably
* Training becomes unstable

---

## 10. Truncated BPTT

Instead of backpropagating till `t = 1`:

```
t = T ‚Üí T-k
```

Benefits:

* Faster training
* More stable gradients
* Reduced vanishing/exploding effect

---

## 11. Basic RNN Implementation (TensorFlow / Keras)

```python
model = Sequential([
    Embedding(input_dim=10000, output_dim=64),
    SimpleRNN(64),
    Dense(1, activation="sigmoid")
])

model.compile(
    optimizer="adam",
    loss="binary_crossentropy",
    metrics=["accuracy"]
)
```

* `Embedding` ‚Üí word vectors
* `SimpleRNN` ‚Üí sequence processing + memory
* `Dense` ‚Üí final prediction

Training:

```python
model.fit(X, y, epochs=5)
```

Keras automatically handles:

* Forward pass
* BPTT
* Gradient updates

---

## 12. Why RNN Fails for Long Sequences

* Repeated Jacobian multiplication
* Memory fades for long dependencies
* Vanishing gradient problem

This led to **LSTM**.

---

## 13. LSTM (Brief Introduction)

**LSTM = Long Short‚ÄëTerm Memory**

Key improvements:

* Introduces **cell state (`c‚Çú`)** ‚Üí long‚Äëterm memory
* Uses **gates** to control information flow

### Two States in LSTM

* `c‚Çú` ‚Üí long‚Äëterm memory (stored using ADDITION)
* `h‚Çú` ‚Üí short‚Äëterm / output memory

Cell update:

```
c‚Çú = f‚Çú ‚äô c‚Çú‚Çã‚ÇÅ + i‚Çú ‚äô g‚Çú
```

Why it works:

* Uses **addition, not repeated multiplication**
* Gradients flow without vanishing

---

## 14. Key Exam‚ÄëReady One‚ÄëLiners

* RNNs process sequential data by maintaining a hidden state across time steps.
* RNNs are trained using Backpropagation Through Time.
* Vanishing gradients occur due to repeated multiplication of Jacobians.
* Truncated BPTT limits backward steps to stabilize training.
* LSTM avoids vanishing gradients using a gated, additive cell state.

---

## 15. Final Mental Model

```
Words ‚Üí Numbers ‚Üí Embeddings ‚Üí RNN Memory ‚Üí Output
```

RNN = short‚Äëterm memory
LSTM = short + long‚Äëterm memory

---

‚úÖ These notes fully cover:

* Concepts
* Architecture
* Math intuition
* BPTT
* Jacobians
* Gradient problems
* Code basics


